# TikTok_Project

Our project invovled analyzing data of over 1,000 comments from our tiktok video discussing whether water was or was not wet. 

This project centered on using an LLM to systematically analyze audience reactions to a TikTok debate video about the classic question: “Is water wet?” The video, posted by @smartypants, features two presenters going back and forth over whether wetness is an inherent property of water or a condition that only applies when water covers another surface. Because the debate hinges on definitions, physical properties, and everyday language, it sparked a large volume of user comments that were perfect for testing how well an LLM could classify opinion-based responses with consistency and reliability.

Our research question was narrowly scoped: How do TikTok users frame their stance in the “Is water wet?” debate, and can an LLM reliably categorize these comments into coherent argumentative types? To answer this, we collected a dataset of 1,000 comments and iteratively refined our coding framework. Early on, our concepts—initially broad categories such as “water is wet,” “water is not wet,” and “meta-comments”—proved too vague for the model. Through multiple coding rounds, we reshaped these into clearer, operationalized concepts: definitional arguments, scientific/chemical reasoning, humor or sarcasm, and off-topic engagement. The more precise the categories became, the better the model performed.

A major component of the project involved improving inter-rater reliability scores. Our initial prompt produced inconsistent agreement across categories, so we modified the instructions to be more explicit, added decision rules, included positive and negative examples, and simplified ambiguous edge cases. These changes significantly increased agreement between human coders and the LLM. Agreeing upon set rules to decide which code numbers to give (between us two) was also incredibly helpful in being able to increase the inter-reliability scores. 
